{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9594200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import glob\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "import tensorflow.keras.applications.mobilenet  \n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "import tensorflow.keras.applications.inception_v3\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tensorflow.keras.preprocessing.image\n",
    "import pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (LSTM, Embedding, \n",
    "    TimeDistributed, Dense, RepeatVector, \n",
    "    Activation, Flatten, Reshape, concatenate,  \n",
    "    Dropout, BatchNormalization)\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "START = \"startseq\"\n",
    "STOP = \"endseq\"\n",
    "EPOCHS = 10\n",
    "USE_INCEPTION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aafc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h}:{m:>02}:{s:>05.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbe812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running Locally\n",
    "\n",
    "if COLAB:\n",
    "    root_captioning = \"/content/drive/My Drive/projects/captions\"\n",
    "else:\n",
    "    root_captioning = \"./data/captions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean/Build Dataset From Flickr\n",
    "\n",
    "null_punct = str.maketrans('', '', string.punctuation)\n",
    "lookup = dict()\n",
    "\n",
    "with open( os.path.join(root_captioning,'Flickr8k_text',\\\n",
    "                        'Flickr8k.token.txt'), 'r') as fp:\n",
    "  \n",
    "  max_length = 0\n",
    "  for line in fp.read().split('\\n'):\n",
    "    tok = line.split()\n",
    "    if len(line) >= 2:\n",
    "      id = tok[0].split('.')[0]\n",
    "      desc = tok[1:]\n",
    "      \n",
    "      # Cleanup description\n",
    "      desc = [word.lower() for word in desc]\n",
    "      desc = [w.translate(null_punct) for w in desc]\n",
    "      desc = [word for word in desc if len(word)>1]\n",
    "      desc = [word for word in desc if word.isalpha()]\n",
    "      max_length = max(max_length,len(desc))\n",
    "      \n",
    "      if id not in lookup:\n",
    "        lookup[id] = list()\n",
    "      lookup[id].append(' '.join(desc))\n",
    "      \n",
    "lex = set()\n",
    "for key in lookup:\n",
    "  [lex.update(d.split()) for d in lookup[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db130bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lookup)) # How many unique words\n",
    "print(len(lex)) # The dictionary\n",
    "print(max_length) # Maximum length of a caption (in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c55e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove embeddings.\n",
    "\n",
    "img = glob.glob(os.path.join(root_captioning,'Flicker8k_Dataset', '*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2359861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = os.path.join(root_captioning,\\\n",
    "            'Flickr8k_text','Flickr_8k.trainImages.txt') \n",
    "train_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\n",
    "test_images_path = os.path.join(root_captioning,\n",
    "            'Flickr8k_text','Flickr_8k.testImages.txt') \n",
    "test_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\n",
    "\n",
    "train_img = []\n",
    "test_img = []\n",
    "\n",
    "for i in img:\n",
    "  f = os.path.split(i)[-1]\n",
    "  if f in train_images: \n",
    "    train_img.append(f) \n",
    "  elif f in test_images:\n",
    "    test_img.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e445ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_images))\n",
    "print(len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92537943",
   "metadata": {},
   "source": [
    "###### Build the sequences. We include a start and stop token at the beginning/end. We will later use the start token to begin the process of generating a caption. Encountering the stop token in the generated text will let us know the process is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d418cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = {k:v for k,v in lookup.items() if f'{k}.jpg' \\\n",
    "                      in train_images}\n",
    "for n,v in train_descriptions.items(): \n",
    "  for d in range(len(v)):\n",
    "    v[d] = f'{START} {v[d]} {STOP}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e5c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8489f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing a Computer Vision Neural Network to Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_INCEPTION:\n",
    "  encode_model = InceptionV3(weights='imagenet')\n",
    "  encode_model = Model(encode_model.input, encode_model.layers[-2].output)\n",
    "  WIDTH = 299\n",
    "  HEIGHT = 299\n",
    "  OUTPUT_DIM = 2048\n",
    "  preprocess_input = \\\n",
    "    tensorflow.keras.applications.inception_v3.preprocess_input\n",
    "else:\n",
    "  encode_model = MobileNet(weights='imagenet',include_top=False)\n",
    "  WIDTH = 224\n",
    "  HEIGHT = 224\n",
    "  OUTPUT_DIM = 50176\n",
    "  preprocess_input = tensorflow.keras.applications.mobilenet.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1cfad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Training Set\n",
    "\n",
    "def encodeImage(img):\n",
    "  # Resize all images to a standard size (specified bythe image \n",
    "  # encoding network)\n",
    "  img = img.resize((WIDTH, HEIGHT), Image.ANTIALIAS)\n",
    "  # Convert a PIL image to a numpy array\n",
    "  x = tensorflow.keras.preprocessing.image.img_to_array(img)\n",
    "  # Expand to 2D array\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  # Perform any preprocessing needed by InceptionV3 or others\n",
    "  x = preprocess_input(x)\n",
    "  # Call InceptionV3 (or other) to extract the smaller feature set for \n",
    "  # the image.\n",
    "  x = encode_model.predict(x) # Get the encoding vector for the image\n",
    "  # Shape to correct form to be accepted by LSTM captioning network.\n",
    "  x = np.reshape(x, OUTPUT_DIM )\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(root_captioning,\"data\",f'train{OUTPUT_DIM}.pkl')\n",
    "if not os.path.exists(train_path):\n",
    "  start = time()\n",
    "  encoding_train = {}\n",
    "  for id in tqdm(train_img):\n",
    "    image_path = os.path.join(root_captioning,'Flicker8k_Dataset', id)\n",
    "    img = tensorflow.keras.preprocessing.image.load_img(image_path, \\\n",
    "            target_size=(HEIGHT, WIDTH))\n",
    "    encoding_train[id] = encodeImage(img)\n",
    "  with open(train_path, \"wb\") as fp:\n",
    "    pickle.dump(encoding_train, fp)\n",
    "  print(f\"\\nGenerating training set took: {hms_string(time()-start)}\")\n",
    "else:\n",
    "  with open(train_path, \"rb\") as fp:\n",
    "    encoding_train = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6bdf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.path.join(root_captioning,\"data\",f'test{OUTPUT_DIM}.pkl')\n",
    "if not os.path.exists(test_path):\n",
    "  start = time()\n",
    "  encoding_test = {}\n",
    "  for id in tqdm(test_img):\n",
    "    image_path = os.path.join(root_captioning,'Flicker8k_Dataset', id)\n",
    "    img = tensorflow.keras.preprocessing.image.load_img(image_path, \\\n",
    "                target_size=(HEIGHT, WIDTH))\n",
    "    encoding_test[id] = encodeImage(img)\n",
    "  with open(test_path, \"wb\") as fp:\n",
    "    pickle.dump(encoding_test, fp)\n",
    "  print(f\"\\nGenerating testing set took: {hms_string(time()-start)}\")\n",
    "else:\n",
    "  with open(test_path, \"rb\") as fp:\n",
    "    encoding_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bdee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)\n",
    "len(all_train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "print('preprocessed words %d ==> %d' % (len(word_counts), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f2e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxtoword = {}\n",
    "wordtoidx = {}\n",
    "\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoidx[w] = ix\n",
    "    idxtoword[ix] = w\n",
    "    ix += 1\n",
    "    \n",
    "vocab_size = len(idxtoword) + 1 \n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfadbe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length +=2\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae3da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, wordtoidx, \\\n",
    "                   max_length, num_photos_per_batch):\n",
    "  # x1 - Training data for photos\n",
    "  # x2 - The caption that goes with each photo\n",
    "  # y - The predicted rest of the caption\n",
    "  x1, x2, y = [], [], []\n",
    "  n=0\n",
    "  while True:\n",
    "    for key, desc_list in descriptions.items():\n",
    "      n+=1\n",
    "      photo = photos[key+'.jpg']\n",
    "      # Each photo has 5 descriptions\n",
    "      for desc in desc_list:\n",
    "        # Convert each word into a list of sequences.\n",
    "        seq = [wordtoidx[word] for word in desc.split(' ') \\\n",
    "               if word in wordtoidx]\n",
    "        # Generate a training case for every possible sequence and outcome\n",
    "        for i in range(1, len(seq)):\n",
    "          in_seq, out_seq = seq[:i], seq[i]\n",
    "          in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "          out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "          x1.append(photo)\n",
    "          x2.append(in_seq)\n",
    "          y.append(out_seq)\n",
    "      if n==num_photos_per_batch:\n",
    "        yield ([np.array(x1), np.array(x2)], np.array(y))\n",
    "        x1, x2, y = [], [], []\n",
    "        n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Glove Embeddings\n",
    "\n",
    "glove_dir = os.path.join(root_captioning,'glove.6B')\n",
    "embeddings_index = {} \n",
    "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print(f'Found {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Neural Network\n",
    "\n",
    "embedding_dim = 200\n",
    "\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in wordtoidx.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4128908",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d10200",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(OUTPUT_DIM,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "caption_model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffb31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849deb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf30e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model.layers[2].set_weights([embedding_matrix])\n",
    "caption_model.layers[2].trainable = False\n",
    "caption_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834189e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Neural Network\n",
    "number_pics_per_bath = 3\n",
    "steps = len(train_descriptions)//number_pics_per_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abfd8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(root_captioning,\"data\",f'caption-model.hdf5')\n",
    "if not os.path.exists(model_path):\n",
    "  for i in tqdm(range(EPOCHS*2)):\n",
    "      generator = data_generator(train_descriptions, encoding_train, \n",
    "                    wordtoidx, max_length, number_pics_per_bath)\n",
    "      caption_model.fit_generator(generator, epochs=1,\n",
    "                    steps_per_epoch=steps, verbose=1)\n",
    "\n",
    "  caption_model.optimizer.lr = 1e-4\n",
    "  number_pics_per_bath = 6\n",
    "  steps = len(train_descriptions)//number_pics_per_bath\n",
    "\n",
    "  for i in range(EPOCHS):\n",
    "      generator = data_generator(train_descriptions, encoding_train, \n",
    "                    wordtoidx, max_length, number_pics_per_bath)\n",
    "      caption_model.fit_generator(generator, epochs=1, \n",
    "                            steps_per_epoch=steps, verbose=1)  \n",
    "  caption_model.save_weights(model_path)\n",
    "  print(f\"\\Training took: {hms_string(time()-start)}\")\n",
    "else:\n",
    "  caption_model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d1af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Captions\n",
    "\n",
    "def generateCaption(photo):\n",
    "    in_text = START\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoidx[w] for w in in_text.split() if w in wordtoidx]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = caption_model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = idxtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == STOP:\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d6f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Performance on Test Data from Flicker\n",
    "\n",
    "for z in range(2): # set higher to see more examples\n",
    "  pic = list(encoding_test.keys())[z]\n",
    "  image = encoding_test[pic].reshape((1,OUTPUT_DIM))\n",
    "  print(os.path.join(root_captioning,'Flicker8k_Dataset', pic))\n",
    "  x=plt.imread(os.path.join(root_captioning,'Flicker8k_Dataset', pic))\n",
    "  plt.imshow(x)\n",
    "  plt.show()\n",
    "  print(\"Caption:\",generateCaption(image))\n",
    "  print(\"_____________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d17d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_test[pic].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Performance on My Photos\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "from matplotlib.pyplot import imshow\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "ROOT = \"https://github.com/jeffheaton/\" + \\\n",
    "        \"t81_558_deep_learning/blob/master/photos/\"\n",
    "urls = [\n",
    "  ROOT+\"annie_dog.jpg?raw=true\",\n",
    "  ROOT+\"landscape.jpg?raw=true\",\n",
    "  ROOT+\"hickory_coat.jpg?raw=true\"\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "  response = requests.get(url)\n",
    "  img = Image.open(BytesIO(response.content))\n",
    "  img.load()\n",
    "\n",
    "  plt.imshow(img)\n",
    "  plt.show()\n",
    "  \n",
    "  response = requests.get(url)\n",
    "\n",
    "  img = encodeImage(img).reshape((1,OUTPUT_DIM))\n",
    "  print(img.shape)\n",
    "  print(\"Caption:\",generateCaption(img))\n",
    "  print(\"_____________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896898c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
